UID,"image","title","institution","speaker","abstract","bio","session"
1,"/static/images/speakers/MihaelaRosca.png","Overview of Unsupervised Learning","DeepMind","Mihaela Rosca","Unsupervised learning is the art and practice of learning structure from data. The challenges of learning from unlabelled data have been met with enthusiasm by generations of researchers captivated by the promise of discovering the core principles behind clustering, representation learning or the modeling of probability distributions. Unsupervised learning also holds the key which unlocks a plethora of applications, from text to speech, compression, summarization, topic modeling, video generation and model based reinforcement learning. In this lecture, we will discuss the what, why and how of unsupervised learning, from the fundamentals to the state of the art.","",3
2,"/static/images/speakers/MikhailBelkin.jpg","From classical statistics to modern ML: the lessons of deep
learning for generalization and optimization","Ohio State University","Mikhail Belkin","""A model with zero training error is overfit to the training data and will typically generalize poorly"" goes statistical textbook wisdom.
Yet, in modern practice over-parametrized deep networks with near perfect  fit on training data still show excellent test performance.
This apparent contradiction points to some troubling cracks in the conceptual foundations of machine learning. While classical analyses rely on a trade-off balancing the complexity of predictors with training error, modern models are best described by interpolation,  where  a predictor is chosen among functions that fit the training data exactly, according  to a certain (implicit or explicit) inductive bias.  I will discuss the nature of the challenge to our understanding and point the way  forward to analyses that account for the empirically observed phenomena and shed light on modern model selection. In particular, I will show how  classical and modern models can  be unified within a single  ""double descent"" risk curve, which subsumes the classical U-shaped bias-variance trade-off curve. 
I will also discuss some important implications of interpolation for optimization,  in terms of ""easy"" optimization, the scarcity of non-global minima and ""essential"" non-convexity.","",7
3,"/static/images/speakers/PetarVelickovic.jpg","Applying Graph Neural Networks at the Bleeding Edge","DeepMind","Petar Velickovic","Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalisations of CNNs to graph-structured data, and neural message-passing approaches. These advances in graph neural networks (GNNs) and related techniques have led to new state-of-the-art results in numerous domains: chemical synthesis, 3D-vision, recommender systems, question answering, continuous control, self-driving and social network analysis. 
With many recent influential papers focussing on toy domains or hand-crafted representative datasets, one might wonder whether such models are seeing real-world application at all. The answer to this question is overwhelmingly positive: bleeding-edge GNN architectures are already being readily deployed in many relevant application spheres. Within my focused lecture, I plan to provide a brief overview of the key GNN concepts and architectures, followed by a direct indication of how they've been deployed in challenging real-world contexts. Time permitting, we will cover applications to: Computational biochemistry and medicine; Traffic simulations and autonomous vehicles; Recommendation systems and fake news detection in social networks;
Understanding, simulating and modelling physical systems. No prior strong knowledge of GNNs is expected or assumed---only basics of supervised deep learning!","",7
4,"/static/images/speakers/GergoOrban.jpg","Discriminative and generative model insights into biological vision","MTA Wigner Research Centre","Gergő Orbán","Learning structured representations of complex data is a challenge both for biological and artificial agents. Architectural similarities between sensory cortices and the most successful tools in computer vision inspire research into the depth of correspondence between artificial and biological perception. How far can this correspondence go? How can we distinguish alternative computational frameworks? How can the computational requirements constrain the feasible computational frameworks? In this lecture I will discuss the contrast between discriminative and generative models of vision. I will explore the power and limitations of deep discriminative models in perceptual tasks and also the potential of hierarchical generative models including deep generative models, such as variational autoencoders.","",7
5,"/static/images/speakers/JacekTabor.jpg","Unsupervised learning: kernels, flows and applications","Jagiellonian University","Jacek Tabor","Part 1: WAE, kernels, and how to compare distributions. In this part we present the introduction to kernels, with a particular attention to characteristic kernels. This enables us to construct the WAE model.
Part 2: Flow-based models. We describe in a more detailed way the flow-based models, in particular NICE and FFJORD.
Part 3: Applications. We present applications of the tools described in the first two slots. In particular we discuss ICA (independent component analysis), outlier analysis and point-cloud models.","",3
6,"/static/images/speakers/RazvanPascanu.jpg","Introduction to Deep Learning","DeepMind","Razvan Pascanu","The lecture is aimed at providing some tools to reason about deep neural networks. We will start by briefly introducing basic concepts such as dataset, supervised learning, loss. We will move towards understanding the mechanics behind the gradient-based learning process that dominates neural network, as well as introducing concepts to understand how neural networks are representing data. We discuss the role of domain knowledge and inductive biases in the deep learning literature, particularly discussing weight sharing and sparsity. We will end with a few open questions on understanding learning and neural networks in general. ","",1
7,"/static/images/speakers/NataliaNeverova.jpg","Computer Vision","Facebook","Natalia Neverova","TBD","",1
8,"/static/images/speakers/DianaBorsa.jpg","Fundamentals of Reinforcement Learning","DeepMind","Diana Borsa","TBD","",2
9,"/static/images/speakers/DoinaPrecup.jpg","Deep Reinforcement Learning","McGill University & DeepMind","Doina Precup","TBD","",2
 
