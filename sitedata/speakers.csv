UID,"image","title","institution","speaker","abstract","bio","session"
1,"/static/images/speakers/MihaelaRosca.png","Overview of Unsupervised Learning","DeepMind","Mihaela Rosca","Unsupervised learning is the art and practice of learning structure from data. The challenges of learning from unlabelled data have been met with enthusiasm by generations of researchers captivated by the promise of discovering the core principles behind clustering, representation learning or the modeling of probability distributions. Unsupervised learning also holds the key which unlocks a plethora of applications, from text to speech, compression, summarization, topic modeling, video generation and model based reinforcement learning. In this lecture, we will discuss the what, why and how of unsupervised learning, from the fundamentals to the state of the art.","","3"
4,"/static/images/speakers/GergoOrban.jpg","Discriminative and generative model insights into biological vision","MTA Wigner Research Centre","Gergő Orbán","Learning structured representations of complex data is a challenge both for biological and artificial agents. Architectural similarities between sensory cortices and the most successful tools in computer vision inspire research into the depth of correspondence between artificial and biological perception. How far can this correspondence go? How can we distinguish alternative computational frameworks? How can the computational requirements constrain the feasible computational frameworks? In this lecture I will discuss the contrast between discriminative and generative models of vision. I will explore the power and limitations of deep discriminative models in perceptual tasks and also the potential of hierarchical generative models including deep generative models, such as variational autoencoders.","","7"
3,"/static/images/speakers/PetarVelickovic.jpg","Applying Graph Neural Networks at the Bleeding Edge","DeepMind","Petar Velickovic","Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalisations of CNNs to graph-structured data, and neural message-passing approaches. These advances in graph neural networks (GNNs) and related techniques have led to new state-of-the-art results in numerous domains: chemical synthesis, 3D-vision, recommender systems, question answering, continuous control, self-driving and social network analysis. 
With many recent influential papers focussing on toy domains or hand-crafted representative datasets, one might wonder whether such models are seeing real-world application at all. The answer to this question is overwhelmingly positive: bleeding-edge GNN architectures are already being readily deployed in many relevant application spheres. Within my focused lecture, I plan to provide a brief overview of the key GNN concepts and architectures, followed by a direct indication of how they've been deployed in challenging real-world contexts. Time permitting, we will cover applications to: Computational biochemistry and medicine; Traffic simulations and autonomous vehicles; Recommendation systems and fake news detection in social networks;
Understanding, simulating and modelling physical systems. No prior strong knowledge of GNNs is expected or assumed---only basics of supervised deep learning!","","7"
2,"/static/images/speakers/MikhailBelkin.jpg","From classical statistics to modern ML: the lessons of deep
learning for generalization and optimization","Ohio State University","Mikhail Belkin","""A model with zero training error is overfit to the training data and will typically generalize poorly"" goes statistical textbook wisdom.
Yet, in modern practice over-parametrized deep networks with near perfect  fit on training data still show excellent test performance.
This apparent contradiction points to some troubling cracks in the conceptual foundations of machine learning. While classical analyses rely on a trade-off balancing the complexity of predictors with training error, modern models are best described by interpolation,  where  a predictor is chosen among functions that fit the training data exactly, according  to a certain (implicit or explicit) inductive bias.  I will discuss the nature of the challenge to our understanding and point the way  forward to analyses that account for the empirically observed phenomena and shed light on modern model selection. In particular, I will show how  classical and modern models can  be unified within a single  ""double descent"" risk curve, which subsumes the classical U-shaped bias-variance trade-off curve. 
I will also discuss some important implications of interpolation for optimization,  in terms of ""easy"" optimization, the scarcity of non-global minima and ""essential"" non-convexity.","","7"
5,"/static/images/speakers/JacekTabor.jpg","Unsupervised learning: kernels, flows and applications","Jagiellonian University","Jacek Tabor","Part 1: WAE, kernels, and how to compare distributions. In this part we present the introduction to kernels, with a particular attention to characteristic kernels. This enables us to construct the WAE model.
Part 2: Flow-based models. We describe in a more detailed way the flow-based models, in particular NICE and FFJORD.
Part 3: Applications. We present applications of the tools described in the first two slots. In particular we discuss ICA (independent component analysis), outlier analysis and point-cloud models.","","3"
6,"/static/images/speakers/RazvanPascanu.jpg","Introduction to Deep Learning","DeepMind","Razvan Pascanu","The lecture is aimed at providing some tools to reason about deep neural networks. We will start by briefly introducing basic concepts such as dataset, supervised learning, loss. We will move towards understanding the mechanics behind the gradient-based learning process that dominates neural network, as well as introducing concepts to understand how neural networks are representing data. We discuss the role of domain knowledge and inductive biases in the deep learning literature, particularly discussing weight sharing and sparsity. We will end with a few open questions on understanding learning and neural networks in general. ","","1"
7,"/static/images/speakers/NataliaNeverova.jpg","Geometry in Computer Vision","Facebook","Natalia Neverova","In this talk, I will focus on the geometrical aspects of computer vision, including 3D understanding and extracting dense point correspondences for deformable object categories. I will also cover some applications of these methods in AR, VR and beyond.","","1"
8,"/static/images/speakers/DianaBorsa.jpg","Fundamentals of Reinforcement Learning","DeepMind","Diana Borsa","In this session we will introduce the Reinforcement Learning (RL) paradigm -- one of the most popular ways for modelling sequential decision making, with long term consequences under potentially unknown dynamics. We will explore what kind of problems can be modelled in the way and will cover some of the most fundamental concepts in RL: Markov Decision Processes, Value Functions, Bellman Equations, (model-free) Prediction and Control problems and building up to associated flagship algorithms. Time-permitting we will touch upon more advanced topics, like exploration and function approximation.","","2"
9,"/static/images/speakers/DoinaPrecup.jpg","Deep Reinforcement Learning","DeepMind","Doina Precup","TBD","","2"
10,"/static/images/speakers/TomasMikolov.jpg","Using Neural Networks for Modeling and Representing Natural Languages","CIIRC, Prague","Tomas Mikolov","Artificial neural networks are nowadays widely used in the area of natural language processing, with applications to sentiment analysis, speech recognition, text classification, machine translation and many others. In this tutorial, I will describe some basic yet powerful concepts that are the building blocks of more advanced techniques, and will also intuitively explain why neural networks work so well for representing natural languages.","","6"
11,"/static/images/speakers/LukaszKaiser.jpg","The Efficient Transformer","Google Research","Lukasz Kaiser","Transformer models have been used in a variety of fields and yield great results on many NLP tasks. But between the BERT, GPT-2, and many other variants, they can be inefficient and it can be hard to apply them. I will introduce a new efficient variant of Transformer called the Reformer. I'll take you through the code that implements it and I will show how it runs at high efficiency and
addresses the main problems or high memory use and low performance on long sequences that limited the use of some Transformers before. I will finish with new applications of Reformer that open up.","","6"
12,"/static/images/speakers/KyunghyunCho.png","QAGS:Question Answering and Generation for evaluating Summarization","New York University","Kyunghyun Cho","The recent progress in neural text generation poses a question whether our evaluation protocols are prepared to deal with these latest text generation systems. This is particularly true in the cases of open-ended text generation, such as story generation, dialogue modeling and summarization, the latter of which I focus in this talk. I introduce a novel paradigm of automated extrinsic evaluation of factual consistency in summarization, called QAGS.","","6"
13,"/static/images/speakers/AlexGraves.jpg","Attention and Memory","DeepMind","Alex Graves","Attention and memory have emerged as two vital new components of deep learning over the last few years. In this lecture I will cover a broad range of contemporary attention mechanisms, including the implicit attention present in any deep network, as well as both discrete and differentiable variants of explicit attention. I will then discuss networks with external memory and explain how attention provides them with selective recall. Lastly, I will review Transformers, a purely attention-based network that has become the standard deep learning architecture for natural language processing.","","8"
14,"/static/images/speakers/RaiaHadsell.jpg|/static/images/speakers/PiotrMirowski.jpg","Challenging the Robots","DeepMind|DeepMind","Raia Hadsell|Piotr Mirowski","Dr. Raia Hadsell is a Research Scientist and the Director of Robotics at DeepMind. Her scholarship and research interests span from embodied robotics to deep reinforcement learning agents, with an emphasis on lifelong, continual and transfer learning and on addressing fundamental challenges in robotics. Raia's PhD thesis (Outstanding Dissertation award, 2009) with Yann LeCun at NYU focused on machine learning using Siamese neural networks (also known as the ""triplet loss"") and on deep learning for mobile robots in the wild. Raia's unique professional journey started with an undergraduate degree in religion and philosophy from Reed College and involved a postdoc at the CMU Robotics Institute and technical management at SRI International. Raia is also the Program Co-Chair for the NeurIPS 2020 conference, which will be fully virtual and innovates with a Broader Impact statement requirement for submissions. Please join us for a Fireside Chat on Wednesday 8 July and ask any question, be it on robotics, RL or on technical careers.","","8"
16,"/static/images/speakers/DavidSilver.jpg|/static/images/speakers/DoinaPrecup.jpg","Challenges in Reinforcement Learning","DeepMind|McGill Univ&DeepMind","David Silver|Doina Precup","","","8"
17,"/static/images/speakers/YoshuaBengio.jpg|/static/images/speakers/ShakirMohamed.jpg","AI for social good","MILa|DeepMind","Yoshua Bengio|Shakir Mohamed","A conversation about what AI for social good is, thinking about and mitigating harms from machine learning and what we can do to encourage more work in general areas of social good. If you have any question related to AI for social good, please use Slido to send them. ","","9"
19,"/static/images/speakers/CarlDoersch.png|/static/images/speakers/VioricaPatraucean.jpg","Supervised Learning","DeepMind|DeepMind","Carl Doersch|Viorica Patraucean","In supervised learning, we are given a dataset with many pairs of inputs and outputs and the goal is to find a (approximate) function which maps each input to the correct output for the examples in that dataset. The hope is that the function would generalize to new examples: i.e., it produces the correct output even for inputs that weren't available while we were searching for the function. In this tutorial, we will focus on how supervised learning is used in computer vision to classify what object is depicted in an image. We'll start with a standard neural network (Resnet-18) as a baseline and we'll train it on CIFAR10 dataset. Next, we'll examine how this process can fail through adversarial examples: images that have been manipulated in ways that are almost invisible to humans, but cause large changes in the network output. Finally, we'll implement a new neural network module called Squeeze-Excite, which improves generalization for CIFAR-10. All exercises will be implemented using Jax and haiku.","","1"
21,"/static/images/speakers/FeryalBehbahani.png|/static/images/speakers/GheorgheComanici.jpg","Reinforcement Learning","DeepMind|DeepMind","Feryal Behbahani|Gheorghe Comanici","The tutorial covers a number of important reinforcement learning (RL) algorithms, including policy iteration, Q-Learning, and Natural Fitted Q. In the first part, we will guide you through the general interaction between RL agents and environments, where the agents ought to take actions in order to maximize returns (i.e. cumulative reward). Next, we will implement Policy Iteration, SARSA, and Q-Learning for a simple tabular environment. The core ideas in the latter will be scaled to more complex MDPs through the use of function approximation. Lastly, we will provide a short introduction to deep reinforcement learning and the DQN algorithm.","","2"
23,"/static/images/speakers/DavidSzepesvari.jpg|/static/images/speakers/StanislawJastrzebski.jpg","Unsupervised Learning","DeepMind|New York University","David Szepesvari|Stanislaw Jastrzebski","Unsupervised learning and generative models are key building blocks of machine learning systems. The tutorial covers a specific class of unsupervised models called autoencoders. We implement and train three variants: the original vanilla autoencoder, variational autoencoder, and time permitting/as homework the recently introduced regularized autoencoders. Along with training these, we examine and investigate the generative modeling capabilities of these three variants. All exercises will be implemented using Jax and haiku.","","3"
27,"/static/images/speakers/DoinaPrecup.jpg|/static/images/speakers/NandodeFreitas.jpg|/static/images/speakers/YannDauphin.png","Panel: Best practices in research","DeepMind|DeepMind|Google Research","Doina Precup|Nando de Freitas|Yann Dauphin","In this panel, we will discuss with leading experts in Machine Learning about best practices in ML research regarding writing, presenting, reviewing a scientific paper, common pitfalls, etc. Send your questions in advance using Slido.","","7"
28,"/static/images/speakers/CarlDoersch.png|/static/images/speakers/VioricaPatraucean.jpg|/static/images/speakers/FeryalBehbahani.png|/static/images/speakers/GheorgheComanici.jpg|/static/images/speakers/DavidSzepesvari.jpg|/static/images/speakers/StanislawJastrzebski.jpg","Tutorial discussions","DeepMind|DeepMind|DeepMind|DeepMind|DeepMind|New York University","Carl Doersch|Viorica Patraucean|Feryal Behbahani|Gheorghe Comanici|David Szepesvari|Stanislaw Jastrzebski","Discuss any questions participants might have after coding the tutorial exercises.","","6"
